{
    "0": {
        "file_id": 0,
        "content": "/README.md",
        "type": "filepath"
    },
    "1": {
        "file_id": 0,
        "content": "This is a README for \"Open Q\\*\", a transformer-based LLM with Q-Learning and A\\* algorithms. It's a work in progress repo.",
        "type": "summary"
    },
    "2": {
        "file_id": 0,
        "content": "# Open Q\\*\n> A transformer-based LLM structurally infused with Q-Learning and A\\* algorithms\nIs this Q\\*? Let's find out!\n**A Work In Progress Repo**",
        "type": "code",
        "location": "/README.md:1-7"
    },
    "3": {
        "file_id": 0,
        "content": "This is a README for \"Open Q\\*\", a transformer-based LLM with Q-Learning and A\\* algorithms. It's a work in progress repo.",
        "type": "comment"
    },
    "4": {
        "file_id": 1,
        "content": "/open_qstar/context.py",
        "type": "filepath"
    },
    "5": {
        "file_id": 1,
        "content": "The code defines two classes, `LearningContext` and `ContextManager`, with instance variables for instructions and parameters. It initializes an empty context dictionary, stores Redis client and SQLAlchemy session objects, switches between contexts based on instruction sets, generates a context key, loads context from storage, and saves the context to storage.",
        "type": "summary"
    },
    "6": {
        "file_id": 1,
        "content": "import pickle\nfrom open_qstar.db.pgsql import ContextModel\nclass LearningContext:\n    def __init__(self, instructions, parameters):\n        self.instructions = instructions\n        self.parameters = parameters\nclass ContextManager:\n    def __init__(self, redis_client, sqlalchemy_session):\n        self.contexts = {}\n        self.current_context = None\n        self.redis_client = redis_client\n        self.sqlalchemy_session = sqlalchemy_session\n    def switch_context(self, new_instruction_set):\n        context_key = self.generate_context_key(new_instruction_set)\n        if context_key in self.contexts:\n            self.current_context = self.contexts[context_key]\n        else:\n            self.current_context = self.load_context_from_storage(context_key, new_instruction_set)\n            self.contexts[context_key] = self.current_context\n    def generate_context_key(self, instruction_set):\n        return hash(tuple(instruction_set))\n    def load_context_from_storage(self, context_key, instruction_set):\n        cached_data = self.redis_client.get(context_key)",
        "type": "code",
        "location": "/open_qstar/context.py:1-28"
    },
    "7": {
        "file_id": 1,
        "content": "This code defines two classes, `LearningContext` and `ContextManager`. The `LearningContext` class has instance variables for instructions and parameters. The `ContextManager` class initializes an empty dictionary of contexts and stores the current context as well as Redis client and SQLAlchemy session objects. It also contains methods to switch between contexts based on instruction sets, generate a context key using tupled instruction sets, and load context from storage (Redis) if it doesn't already exist in memory.",
        "type": "comment"
    },
    "8": {
        "file_id": 1,
        "content": "        if cached_data:\n            return pickle.loads(cached_data)\n        context_data = self.sqlalchemy_session.query(ContextModel).filter_by(key=context_key).first()\n        if context_data:\n            self.redis_client.set(context_key, context_data.data)\n            return pickle.loads(context_data.data)\n        return LearningContext(instruction_set, self.initialize_parameters())\n    def save_context_to_storage(self, context_key, context_data):\n        # Serialize and save the context data to PostgreSQL and Redis\n        serialized_data = pickle.dumps(context_data)\n        self.redis_client.set(context_key, serialized_data)\n        db_context = ContextModel(key=context_key, data=serialized_data)\n        self.sqlalchemy_session.add(db_context)\n        self.sqlalchemy_session.commit()\n    def initialize_parameters(self):\n        # TODO Initialize model parameters for a new context\n        # ...\n        return parameters",
        "type": "code",
        "location": "/open_qstar/context.py:29-51"
    },
    "9": {
        "file_id": 1,
        "content": "This code initializes a LearningContext, retrieves and caches context data from Redis or PostgreSQL, and defines methods to save the context to storage.",
        "type": "comment"
    },
    "10": {
        "file_id": 2,
        "content": "/open_qstar/graph_manager.py",
        "type": "filepath"
    },
    "11": {
        "file_id": 2,
        "content": "This code defines a GraphManager class that utilizes NetworkX to manage and calculate shortest paths between nodes in a graph. It stores nodes as states and adds/updates edges with negative rewards as costs.",
        "type": "summary"
    },
    "12": {
        "file_id": 2,
        "content": "import networkx as nx\nclass GraphManager:\n    def __init__(self):\n        self.graph = nx.Graph()\n    def add_or_update_edge(self, state, next_state, reward):\n        edge_weight = -reward  # Using negative reward as cost\n        if not self.graph.has_node(state):\n            self.graph.add_node(state)\n        if not self.graph.has_node(next_state):\n            self.graph.add_node(next_state)\n        self.graph.add_edge(state, next_state, weight=edge_weight)\n    def shortest_path(self, source, target):\n        return nx.shortest_path(self.graph, source=source, target=target, weight='weight')",
        "type": "code",
        "location": "/open_qstar/graph_manager.py:1-16"
    },
    "13": {
        "file_id": 2,
        "content": "This code defines a GraphManager class that utilizes NetworkX to manage and calculate shortest paths between nodes in a graph. It stores nodes as states and adds/updates edges with negative rewards as costs.",
        "type": "comment"
    },
    "14": {
        "file_id": 3,
        "content": "/open_qstar/instruction_tuner.py",
        "type": "filepath"
    },
    "15": {
        "file_id": 3,
        "content": "The code uses Redis and SQLAlchemy connections to create a hierarchical instruction tuner that retains learning across multiple instruction tunings, with methods for generating keys, creating contexts, loading from database, and updating Redis cache.",
        "type": "summary"
    },
    "16": {
        "file_id": 3,
        "content": "import redis\nfrom sqlalchemy import create_engine\n# Retain learning across instruction tunings\nclass HierarchicalInstructionTuner:\n    def __init__(self, redis_conn, sql_conn):\n        self.contexts = {}  # Dictionary to store learning contexts\n        self.current_context = None\n        self.redis_conn = redis_conn  # Redis connection for recent data\n        self.sql_conn = sql_conn  # SQLAlchemy connection for persisted data\n    def switch_context(self, new_instruction_set):\n        # Switch to a different context based on new_instruction_set\n        context_key = self.generate_context_key(new_instruction_set)\n        if context_key not in self.contexts:\n            # Load context from SQL if available, else create new\n            self.contexts[context_key] = self.load_context_from_sql(context_key) or self.create_new_context(new_instruction_set)\n        self.current_context = self.contexts[context_key]\n        # Update Redis cache with the current context\n        self.update_redis_cache(self.current_context)",
        "type": "code",
        "location": "/open_qstar/instruction_tuner.py:1-20"
    },
    "17": {
        "file_id": 3,
        "content": "Creates a hierarchical instruction tuner with Redis and SQLAlchemy connections for retaining learning across instruction tunings.",
        "type": "comment"
    },
    "18": {
        "file_id": 3,
        "content": "    def generate_context_key(self, instruction_set):\n        # Generate a unique key for the instruction set (implementation dependent)\n        pass\n    def create_new_context(self, instruction_set):\n        # Create a new learning context for the given instruction set\n        pass\n    def load_context_from_sql(self, context_key):\n        # Load context from SQL database\n        pass\n    def update_redis_cache(self, context):\n        # Update Redis cache with the current context\n        pass\n# Initialize connections\nredis_conn = redis.Redis(host='localhost', port=6379, db=0)\nsql_conn = create_engine('postgresql://user:password@localhost/dbname')\n# Initialize Hierarchical Instruction Tuner\ntuner = HierarchicalInstructionTuner(redis_conn, sql_conn)",
        "type": "code",
        "location": "/open_qstar/instruction_tuner.py:22-43"
    },
    "19": {
        "file_id": 3,
        "content": "The code initializes Redis and SQL connections, and then initializes an instance of HierarchicalInstructionTuner class using these connections. The class has methods to generate a unique key for the instruction set, create a new learning context for the given instruction set, load the context from the SQL database, and update the Redis cache with the current context.",
        "type": "comment"
    },
    "20": {
        "file_id": 4,
        "content": "/open_qstar/qstar.py",
        "type": "filepath"
    },
    "21": {
        "file_id": 4,
        "content": "The code introduces CoreTransformerModel and DynamicCostQLearning classes for dynamic cost estimation using Transformers library, implements QStarModel for goal-oriented tasks, trains a Q-learning agent with A* algorithm, iteratively processes data streams, computes adjusted losses, performs model updates in each batch, logs progress, handles errors, and calculates average loss.",
        "type": "summary"
    },
    "22": {
        "file_id": 4,
        "content": "import sys\nimport torch\nfrom torch import nn\nfrom transformers import AutoModel\nimport networkx as nx\nfrom loguru import logger\nfrom open_qstar.graph_manager import GraphManager\nlogger.remove()\nlogger.add(sys.stderr, level=\"INFO\")\nclass CoreTransformerModel(nn.Module):\n    def __init__(self, model_name='bert-base-uncased'):\n        super().__init__()\n        self.base_model = AutoModel.from_pretrained(model_name)\n    def forward(self, input_ids, attention_mask):\n        return self.base_model(input_ids, attention_mask=attention_mask)\nclass DynamicCostQLearning(nn.Module):\n    def __init__(self, state_dim, action_dim, learning_rate=0.01):\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.learning_rate = learning_rate\n        self.q_table = torch.zeros(state_dim, action_dim)\n        self.state_graph = nx.Graph()\n        self.graph_manager = GraphManager()\n    def compute_loss_estimate(self, state, action, reward, next_state, instruction_context):\n        # Use the instruction context to adjust the loss estimation",
        "type": "code",
        "location": "/open_qstar/qstar.py:1-31"
    },
    "23": {
        "file_id": 4,
        "content": "Code imports necessary libraries and defines two classes: CoreTransformerModel and DynamicCostQLearning. The first class is a wrapper for the Transformers library's AutoModel, while the second class represents a Q-learning model for dynamic cost estimation with adjustable loss estimation using instruction context.",
        "type": "comment"
    },
    "24": {
        "file_id": 4,
        "content": "        # For example, certain instructions might amplify the importance of rewards or penalties\n        td_target = reward + self.q_table[next_state, torch.argmax(self.q_table[next_state])]\n        td_error = td_target - self.q_table[state, action]\n        adjusted_loss = abs(td_error) * instruction_context  # Example of adjustment\n        return adjusted_loss\n    def update_policy_and_graph(self, state, action, reward, next_state):\n        # Update Q-table\n        best_next_action = torch.argmax(self.q_table[next_state])\n        td_target = reward + self.q_table[next_state, best_next_action]\n        td_error = td_target - self.q_table[state, action]\n        self.q_table[state, action] += self.learning_rate * td_error\n        # Update the graph\n        self.update_graph(state, action, next_state, reward)\n        self.graph_manager.add_or_update_edge(state, next_state, reward)\n    def update_graph(self, state, action, next_state, reward):\n        if not self.state_graph.has_node(state):\n            self.state_graph.add_node(state)",
        "type": "code",
        "location": "/open_qstar/qstar.py:32-51"
    },
    "25": {
        "file_id": 4,
        "content": "This code updates the Q-table, policy, and graph in a reinforcement learning agent. It calculates the TD error and adjusts the loss based on an instruction context. The state, action, reward, and next_state are used to update these elements.",
        "type": "comment"
    },
    "26": {
        "file_id": 4,
        "content": "        if not self.state_graph.has_node(next_state):\n            self.state_graph.add_node(next_state)\n        edge_weight = -reward\n        self.state_graph.add_edge(state, next_state, weight=edge_weight)\ndef a_star_heuristic(state, goal_state):\n    return torch.norm(state - goal_state)\nclass InstructionEncoder(nn.Module):\n    def __init__(self, state_dim):\n        super().__init__()\n        self.encoder = nn.Linear(state_dim, state_dim)\n    def forward(self, instruction_state):\n        return self.encoder(instruction_state)\n    def encode_for_loss(self, instruction_state):\n        # This method encodes instructions specifically for the context of loss estimation\n        # For example, certain instructions might indicate that certain outcomes are more \n        # critical than others, which can be reflected in the loss computation\n        encoded_loss_context = self.encoder(instruction_state)  # Example implementation\n        return encoded_loss_context\ndef update_goal_state_based_on_instruction(goal_state, instruction):",
        "type": "code",
        "location": "/open_qstar/qstar.py:52-75"
    },
    "27": {
        "file_id": 4,
        "content": "Creating a new node in state graph if it does not exist, then adding an edge between current and next states with negative reward. A* heuristic for calculating distance from state to goal_state using norm. InstructionEncoder class for encoding instruction states. encode_for_loss method for encoding instructions specific to loss estimation. update_goal_state_based_on_instruction function updates the goal_state based on provided instruction.",
        "type": "comment"
    },
    "28": {
        "file_id": 4,
        "content": "    return goal_state + instruction\nclass QStarModel(nn.Module):\n    def __init__(self, model_name='bert-base-uncased', state_dim=1024, action_dim=30522):\n        super().__init__()\n        self.transformer_model = CoreTransformerModel(model_name)\n        self.q_learning_module = DynamicCostQLearning(state_dim, action_dim)\n        self.instruction_encoder = InstructionEncoder(state_dim)\n    def forward(self, input_ids, attention_mask, instruction_ids, goal_state, reward=None, next_state=None):\n        transformer_outputs = self.transformer_model(input_ids, attention_mask)\n        current_state = transformer_outputs.last_hidden_state.mean(dim=1)\n        instruction_state = self.transformer_model(instruction_ids, attention_mask=None).last_hidden_state.mean(dim=1)\n        encoded_instruction = self.instruction_encoder(instruction_state)\n        dynamic_goal_state = update_goal_state_based_on_instruction(goal_state, encoded_instruction)\n        heuristic_value = a_star_heuristic(current_state, dynamic_goal_state)",
        "type": "code",
        "location": "/open_qstar/qstar.py:76-93"
    },
    "29": {
        "file_id": 4,
        "content": "The code defines a QStarModel class which is a neural network model for solving goal-oriented tasks. It uses a pretrained transformer model (CoreTransformerModel) and a dynamic cost Q learning module (DynamicCostQLearning) to predict actions based on input and instruction states. The InstructionEncoder is used to encode the instruction state. The forward function takes input_ids, attention_mask, instruction_ids, goal_state, reward (optional), and next_state (optional) as inputs and returns encoded_instruction, dynamic_goal_state, heuristic_value based on these inputs.",
        "type": "comment"
    },
    "30": {
        "file_id": 4,
        "content": "        action = self.decide_action_with_a_star(current_state, dynamic_goal_state)\n        instruction_state = self.transformer_model(instruction_ids, attention_mask=None).last_hidden_state.mean(dim=1)\n        instruction_context = self.instruction_encoder.encode_for_loss(instruction_state)\n        # Compute loss estimate considering the instruction context\n        model_output = self.q_learning_module.compute_loss_estimate(current_state, action, reward, next_state, instruction_context)\n        loss_estimate = self.dynamic_loss_function(model_output, instruction_context, context_parameters)\n        return {'action': action, 'loss': loss_estimate}\n    def dynamic_loss_function(self, model_output, instruction_context, context_parameters):\n        # Example: Adjust the loss computation based on the current instruction context\n        # and context-specific parameters\n        base_loss = self.base_loss_computation(model_output)  # Define base loss computation\n        context_adjustment = self.context_specific_adjustment(instruction_context, context_parameters)",
        "type": "code",
        "location": "/open_qstar/qstar.py:94-108"
    },
    "31": {
        "file_id": 4,
        "content": "This code is part of a Q-learning agent that takes in current state, dynamic goal state, and an instruction. It decides an action based on the current state and dynamic goal state using the decide_action_with_a_star method. It then transforms the instruction into a meaningful contextual representation using the transformer model. The Q-learning module is used to compute the loss estimate considering the instruction context. The dynamic_loss_function adjusts the base loss computation with the instruction context and context-specific parameters.",
        "type": "comment"
    },
    "32": {
        "file_id": 4,
        "content": "        adjusted_loss = base_loss * context_adjustment\n        return adjusted_loss\n    def base_loss_computation(self, model_output):\n        # Define how the base loss is computed from the model's output\n        # ...\n        pass\n    def context_specific_adjustment(self, instruction_context, context_parameters):\n        # Adjust the loss based on the instruction context and context-specific parameters\n        # ...\n        return adjustment_factor\n    def decide_action_with_a_star(self, current_state, goal_state):\n        path = self.q_learning_module.graph_manager.shortest_path(current_state, goal_state)\n        next_action = path[1] if len(path) > 1 else None\n        return next_action\ndef process_stream_data(model, data_stream, optimizer, context_manager):\n    total_loss = 0.0\n    for i, data in enumerate(data_stream):\n        try:\n            input_ids, attention_mask, instruction_ids, goal_state, reward, next_state = data\n            context_parameters = context_manager.current_context.parameters\n",
        "type": "code",
        "location": "/open_qstar/qstar.py:109-133"
    },
    "33": {
        "file_id": 4,
        "content": "This code defines a function to compute the adjusted loss based on base loss and context adjustment. It also has functions for computing base loss, context-specific adjustment, and deciding action with A* algorithm. Additionally, there's a process_stream_data function that iterates through data stream, computes losses, and uses context manager for context parameters.",
        "type": "comment"
    },
    "34": {
        "file_id": 4,
        "content": "            model_output = model(input_ids, attention_mask, instruction_ids, goal_state, reward, next_state, context_parameters)\n            loss = model_output['loss']\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            if i % 100 == 0:\n                logger.info(f\"Stream Batch {i}, Current Loss: {loss.item()}\")\n        except Exception as e:\n            logger.error(f\"Error in continuous learning process at batch {i}: {e}\")\n            continue\n    average_loss = total_loss / len(data_stream)\n    logger.info(f\"Continuous learning process completed. Average Loss: {average_loss}\")",
        "type": "code",
        "location": "/open_qstar/qstar.py:133-149"
    },
    "35": {
        "file_id": 4,
        "content": "This code is performing model training in a loop for each batch of data from the data stream. It calculates the loss, updates the model using backpropagation and optimization, logs progress, and keeps track of the total loss. If an error occurs during training at a specific batch, it logs the error and continues. After completing all batches, it calculates and logs the average loss for the entire continuous learning process.",
        "type": "comment"
    },
    "36": {
        "file_id": 5,
        "content": "/pyproject.toml",
        "type": "filepath"
    },
    "37": {
        "file_id": 5,
        "content": "This code is a \"pyproject.toml\" configuration file for a Python project named \"open-qstar\". It specifies the version, dependencies and build system requirements.",
        "type": "summary"
    },
    "38": {
        "file_id": 5,
        "content": "[tool.poetry]\nname = \"open-qstar\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"estill01 <ehs.stillman@gmail.com>\"]\nreadme = \"README.md\"\npackages = [{include = \"open_qstar\"}]\n[tool.poetry.dependencies]\npython = \"^3.10\"\ntorch = \"^2.1.1\"\ntransformers = \"^4.35.2\"\nloguru = \"^0.7.2\"\nredis = \"^5.0.1\"\nsqlalchemy = \"^2.0.23\"\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"",
        "type": "code",
        "location": "/pyproject.toml:1-20"
    },
    "39": {
        "file_id": 5,
        "content": "This code is a \"pyproject.toml\" configuration file for a Python project named \"open-qstar\". It specifies the version, dependencies and build system requirements.",
        "type": "comment"
    }
}